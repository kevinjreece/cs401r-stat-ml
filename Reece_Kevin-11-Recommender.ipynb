{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Writeup\n",
    "### 1. A discussion of your exploration of the dataset\n",
    "My exploration of the data was pretty simple. I looked at the histrogram of all the ratings to get a sense for the general spread. By looking at the histogram, I saw that the ratings resemble a simple normal distribution with a mean around 3.5 and a pretty low variance. In addition to looking at the histrogram, I also printed out the first and last dozen rows (approximately) in order to see what kind of features I was working with. I knew I was going to want to include some information about movies in my model without just using the id because that would be useless. So I looked into getting information about each movie. I paid particular attention to the genres of each movie. I was surprised to find that there were only 20 genres for all hundreds of thousands. I also saw the tags. Since there were hundreds of tags compared to the 20 genres, I decided it would be simpler and easier to work with genres when comparing movies rather than tags.\n",
    "\n",
    "### 2. A clear technical description of your approach\n",
    "I decided early on that I would use an MLP to try to solve this problem. I did this for three reasons. First and foremost, I see ratings as a regression problem, which MLP do well on. Second, I understand MLP better than most other machine learning concepts. And third, I knew that I could use the Keras library to do the real technical heavy lifting for me so that I could focus more on the concepts.\n",
    "\n",
    "Although my goal was to use an MLP, I started by performing the most simple prediction in order to have a baseline to work from. This naive prediction algorithm was to simply assign every user/movie pair the mean score of all ratings in my training set. This naive approach did surprisingly well with an RMSE as low as 0.998.\n",
    "\n",
    "After I had this naive baseline, I began to plan my MLP model. Before spending too much time working on complicated inputs features for the MLP, I decided to try the most basic inputs possible: the mean rating the user in question gave to movies and the mean rating the movie in question had received. With only these two features as inputs, I decided my first MLP could simply be a perceptron with 2 inputs and 1 output. The perceptron would essentially be trying to find the best way to combine a user's mean rating given and movie's mean rating received in order to predict the rating that user would give that movie. This basic perceptron also did surprising well with an RMSE as low as 0.824 on my testing data.\n",
    "\n",
    "I was very surprised that my simple perceptron was able to achieve this accuracy. Based on my knowledge of other students' results, I was satisfied with my results and decided to end my experiments and not pursue a more complicated MLP predictor. This was largely a factor of time. Since I am already late, I decided it was more important for me to turn in what I had done than spend more time experimenting.\n",
    "\n",
    "However, I had put a lot of thought into the feature space I would use for an MLP. I wanted to include in the input space a vector of 20 numbers (0, 1) to represent the genres the movie belonged in. I would then include another vector of 20 numbers that would be the mean rating the user gave to movies from each genre. For any genre that a user hadn't rated, I would have set the value to the mean rating value fromt he entire training set. By using these two genre vectors (one for the movie and one for the user) I hoped to capture unough information about both to allow the MLP to find patterns necessary for predicting a user's rating of a movie.\n",
    "\n",
    "As a final note for this section, to separate my data into training and testing sets I just used the code provided in the hints section of the spec that made a random subset of 85,000 values from the training data to be the test set. The remaining values served as my true training values.\n",
    "\n",
    "### 3. An analysis of how your approach worked on the dataset\n",
    "As stated above, the final RMSE on my test set was 0.824.\n",
    "\n",
    "I do not believe that I overfit the data because my RMSE on my test and validation sets (0.830 and 0.824, respectively) was significantly lower than the RMSE on my training set (0.928).\n",
    "\n",
    "I ended up not using the first algorithm I had originally planned on using (the more robust MLP). Instead, I tried starting with simpler solutions and found that they were quite effective dispite their simplicity. I decided to apply Occham's Razor and assume that a simpler model that performs well is preferable to a larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/Users/kevinjreece/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import arff\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "import kjr_tools\n",
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def measurePredictions(predictions, targets):\n",
    "    sse = 0\n",
    "    for i, (index, t) in enumerate(targets.iterrows()):\n",
    "        sse += (t.rating - predictions[i]) ** 2\n",
    "    mse = sse / len(targets)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ur = pandas.read_csv('data/movie_training_data/user_ratedmovies_train.dat','\\t')\n",
    "predictions = pandas.read_csv('data/predictions.dat','\\t')\n",
    "movie_actors = pandas.read_csv('data/movie_training_data/movie_actors.dat', '\\t')\n",
    "movie_genres = pandas.read_csv('data/movie_training_data/movie_genres.dat', '\\t')\n",
    "movie_tags = pandas.read_csv('data/movie_training_data/movie_tags.dat', '\\t')\n",
    "movies = pandas.read_csv('data/movie_training_data/movies.dat', '\\t')\n",
    "tags = pandas.read_csv('data/movie_training_data/tags.dat', '\\t')\n",
    "user_tags = pandas.read_csv('data/movie_training_data/user_taggedmovies.dat', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAECCAYAAADkaECYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEJKkov8D4X0aFGseip8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114bc2650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist( ur['rating'] )\n",
    " \n",
    "# create a test/train split\n",
    " \n",
    "all_inds = np.random.permutation( range(0,len(ur)) )\n",
    "test_inds = all_inds[0:85000]\n",
    "train_inds = all_inds[85000:len(ur)]\n",
    " \n",
    "ur_test = ur.iloc[ test_inds ]\n",
    "ur_train = ur.iloc[ train_inds ]\n",
    "\n",
    "all_train_inds = np.random.permutation(range(0, len(ur_train)))\n",
    "validation_inds = all_train_inds[0:150000]\n",
    "learn_inds = all_train_inds[150000: len(ur_train)]\n",
    "\n",
    "ur_validation = ur_train.iloc[validation_inds]\n",
    "ur_learn = ur_train.iloc[learn_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_movies = movies.iloc[-1]['id'] + 1\n",
    "genres_set = set(movie_genres['genre'])\n",
    "num_genres = len(genres_set)\n",
    "num_users = max(ur_train.userID) + 1\n",
    "\n",
    "genres_dict = {}\n",
    "for i, g in enumerate(genres_set):\n",
    "    genres_dict[g] = i\n",
    "movie_genre_vectors = np.zeros((num_movies, num_genres))\n",
    "for index, row in movie_genres.iterrows():\n",
    "    movie_genre_vectors[row['movieID']-1, genres_dict[row['genre']]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Naive solution\n",
    "#####\n",
    "train_ratings = ur_train.rating\n",
    "train_mean = np.mean(train_ratings)\n",
    "naive_predictions = [train_mean] * len(ur_test)\n",
    "\n",
    "print measurePredictions(naive_predictions, ur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# average rating each user gave in the training dataset\n",
    "user_mean_ratings = np.array([np.mean(ur_train[ur_train.userID == user_id].rating) \\\n",
    "                              for user_id in range(num_users)])\n",
    "# average rating each movie received in the training dataset\n",
    "movie_mean_ratings = np.array([np.mean(ur_train[ur_train.movieID == movie_id].rating) \\\n",
    "                               for movie_id in range(num_movies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_mean_ratings[np.isnan(user_mean_ratings)] = np.mean(ur_learn.rating)\n",
    "movie_mean_ratings[np.isnan(movie_mean_ratings)] = np.mean(ur_learn.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ur_learn_targets = ur_learn[['rating']].values\n",
    "ur_validation_targets = ur_validation[['rating']].values\n",
    "ur_train_targets = ur_train[['rating']].values\n",
    "ur_test_targets = ur_test[['rating']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learn\n",
    "user_mean_per_learn_input = np.atleast_2d([user_mean_ratings[user_id] \\\n",
    "                                           for user_id in ur_learn.userID.values]).T\n",
    "movie_mean_per_learn_input = np.atleast_2d([movie_mean_ratings[movie_id] \\\n",
    "                                            for movie_id in ur_learn.movieID.values]).T\n",
    "ur_learn_inputs = np.concatenate((user_mean_per_learn_input, movie_mean_per_learn_input), axis=1)\n",
    "\n",
    "# validation\n",
    "user_mean_per_validation_input = np.atleast_2d([user_mean_ratings[user_id] \\\n",
    "                                                for user_id in ur_validation.userID.values]).T\n",
    "movie_mean_per_validation_input = np.atleast_2d([movie_mean_ratings[movie_id] \\\n",
    "                                                 for movie_id in ur_validation.movieID.values]).T\n",
    "ur_validation_inputs = np.concatenate((user_mean_per_validation_input, movie_mean_per_validation_input), axis=1)\n",
    "\n",
    "# train\n",
    "user_mean_per_train_input = np.atleast_2d([user_mean_ratings[user_id] \\\n",
    "                                                for user_id in ur_train.userID.values]).T\n",
    "movie_mean_per_train_input = np.atleast_2d([movie_mean_ratings[movie_id] \\\n",
    "                                                 for movie_id in ur_train.movieID.values]).T\n",
    "ur_train_inputs = np.concatenate((user_mean_per_train_input, movie_mean_per_train_input), axis=1)\n",
    "\n",
    "# test\n",
    "user_mean_per_test_input = np.atleast_2d([user_mean_ratings[user_id] \\\n",
    "                                                for user_id in ur_test.userID.values]).T\n",
    "movie_mean_per_test_input = np.atleast_2d([movie_mean_ratings[movie_id] \\\n",
    "                                                 for movie_id in ur_test.movieID.values]).T\n",
    "ur_test_inputs = np.concatenate((user_mean_per_test_input, movie_mean_per_test_input), axis=1)\n",
    "\n",
    "# predictions\n",
    "user_mean_per_prediction_input = np.atleast_2d([user_mean_ratings[user_id] \\\n",
    "                                           for user_id in predictions.userID.values]).T\n",
    "movie_mean_per_prediction_input = np.atleast_2d([movie_mean_ratings[movie_id] \\\n",
    "                                            for movie_id in predictions.movieID.values]).T\n",
    "prediction_inputs = np.concatenate((user_mean_per_prediction_input, movie_mean_per_prediction_input), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# Basic perceptron\n",
    "#####\n",
    "perceptron_model = Sequential()\n",
    "perceptron_model.add(Dense(1, input_dim=2))\n",
    "perceptron_model.compile(loss='mse', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10000/10000 [==============================] - 7s - loss: 0.8882 - acc: 0.2575     \n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 7s - loss: 0.8620 - acc: 0.2642     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126b7dd50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_model.fit(ur_train_inputs[:10000], ur_train_targets[:10000],\n",
    "          nb_epoch=2,\n",
    "          batch_size=1,\n",
    "          show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000/150000 [==============================] - 14s    \n"
     ]
    }
   ],
   "source": [
    "perceptron_score_validation = perceptron_model.evaluate(ur_validation_inputs, ur_validation_targets, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000/85000 [==============================] - 9s     \n"
     ]
    }
   ],
   "source": [
    "perceptron_score_test = perceptron_model.evaluate(ur_test_inputs, ur_test_targets, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.688463765789\n"
     ]
    }
   ],
   "source": [
    "print perceptron_score_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679243177927\n"
     ]
    }
   ],
   "source": [
    "print perceptron_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_predictions = np.array([perceptron_model.predict(np.atleast_2d(i)) for i in prediction_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_predictions[final_predictions > 5] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('reece_predictions.txt', 'w')\n",
    "for i, p in enumerate(final_predictions):\n",
    "    f.write(\"{},{}\\n\".format(i, p[0, 0]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Code I started for the complicated MLP but never used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_genre_matrix = []\n",
    "for i in range(num_users):\n",
    "    user_genre_matrix.append([])\n",
    "    \n",
    "for i, row in ur_train.iterrows():\n",
    "    user_id = int(row.userID)\n",
    "    movie_id = int(row.movieID)\n",
    "    rating = row.rating\n",
    "    movie_genre_vector = movie_genre_vectors[movie_id] * rating\n",
    "    user_genre_matrix[user_id].append(movie_genre_vector)\n",
    "    \n",
    "user_genre_matrix = np.array(map(np.array, user_genre_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
